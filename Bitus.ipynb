{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "140c0ace-889c-4640-a83c-3bda65334fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184e3de-b92d-426b-9262-e2bf4442f1ba",
   "metadata": {},
   "source": [
    "# data sorting and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eaf0dc33-24fd-448c-ac7e-403f3e39f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/events.csv\")\n",
    "event2id = {\"view\": 0, \"addtocart\": 1, \"transaction\": 2}\n",
    "df[\"event\"] = df[\"event\"].map(event2id)\n",
    "df = df.sort_values([\"visitorid\", \"timestamp\"])\n",
    "\n",
    "# Group events by user\n",
    "user_events = defaultdict(list)\n",
    "for row in df.itertuples():\n",
    "    user_events[row.visitorid].append((row.timestamp, row.event, row.itemid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da183153-6a73-4485-bfc6-cd2a868d2a23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split users into train/val/test\n",
    "user_ids = list(user_events.keys())\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(user_ids)\n",
    "n_total = len(user_ids)\n",
    "train_users = user_ids[:int(0.7 * n_total)]\n",
    "val_users = user_ids[int(0.7 * n_total):int(0.85 * n_total)]\n",
    "test_users = user_ids[int(0.85 * n_total):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f0d064a1-3aed-431b-b9b1-6402739b5b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985305"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79a26a-cf4b-4a11-bb9b-6747390cb775",
   "metadata": {},
   "source": [
    "# padding and truncating of user event sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4172653-3930-4f0b-8c70-0fa2e69b5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sequences\n",
    "def build_sequences(user_ids, user_sequences, seq_len=50):\n",
    "    X, y = [], []\n",
    "    for uid in user_ids:\n",
    "        events = user_sequences[uid]\n",
    "        for i in range(1, len(events)):\n",
    "            seq = events[max(0, i - seq_len):i]\n",
    "            pad_len = seq_len - len(seq)\n",
    "            if pad_len > 0:\n",
    "                seq = [(0, 0, 0)] * pad_len + seq\n",
    "            timestamps = [e[0] for e in seq]\n",
    "            event_types = [e[1] for e in seq]\n",
    "            item_ids = [e[2] for e in seq]\n",
    "            time_diffs = [0] + [timestamps[j] - timestamps[j-1] for j in range(1, len(timestamps))]\n",
    "            features = list(zip(event_types, time_diffs, item_ids))\n",
    "            X.append(features)\n",
    "            y.append(events[i][1])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = build_sequences(train_users, user_events)\n",
    "X_val, y_val = build_sequences(val_users, user_events)\n",
    "X_test, y_test = build_sequences(test_users, user_events)\n",
    "\n",
    "# Build vocab for item ids\n",
    "all_item_ids = {itemid for seq in X_train + X_val + X_test for (_, _, itemid) in seq}\n",
    "item2id = {item: idx + 1 for idx, item in enumerate(sorted(all_item_ids))}\n",
    "item2id[0] = 0\n",
    "num_item_ids = len(item2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b25333-b9b0-4454-89e4-0edee8917b49",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b900f39-bc37-4ec6-8b72-65114fe50134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class EventSequenceDataset(Dataset):\n",
    "    def __init__(self, X, y, item2id):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.item2id = item2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event_types = torch.tensor([e[0] for e in self.X[idx]], dtype=torch.long)\n",
    "        time_diffs = torch.tensor([e[1] for e in self.X[idx]], dtype=torch.float32)\n",
    "        item_ids = torch.tensor([self.item2id.get(e[2], 0) for e in self.X[idx]], dtype=torch.long)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return event_types, time_diffs, item_ids, label\n",
    "\n",
    "train_dataset = EventSequenceDataset(X_train, y_train, item2id)\n",
    "val_dataset = EventSequenceDataset(X_val, y_val, item2id)\n",
    "test_dataset = EventSequenceDataset(X_test, y_test, item2id)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a009dde-f5c4-48e7-9a60-b98c8564ada6",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "253c7145-94e0-48e3-9e88-ae1013009109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BehaviorRNN(\n",
       "  (event_embedding): Embedding(3, 32)\n",
       "  (item_embedding): Embedding(150789, 32)\n",
       "  (time_embedding): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (rnn): GRU(96, 64, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BehaviorRNN(nn.Module):\n",
    "    def __init__(self, event_vocab_size, item_vocab_size, embed_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.event_embedding = nn.Embedding(event_vocab_size, embed_dim)\n",
    "        self.item_embedding = nn.Embedding(item_vocab_size, embed_dim)\n",
    "        self.time_embedding = nn.Linear(1, embed_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(embed_dim * 3, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, event_types, time_diffs, item_ids):\n",
    "        e_embed = self.event_embedding(event_types)\n",
    "        i_embed = self.item_embedding(item_ids)\n",
    "        t_embed = self.time_embedding(time_diffs.unsqueeze(-1))\n",
    "\n",
    "        x = torch.cat([e_embed, t_embed, i_embed], dim=-1)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]  # use the last time step\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# Instantiate model\n",
    "model = BehaviorRNN(event_vocab_size=3, item_vocab_size=num_item_ids + 1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a2650-98df-4713-9be0-f777f584a01e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "38b0b47e-81ed-4171-a0f3-c9dcda316220",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "# to handle imbalance class problems\n",
    "train_counts = np.array([893279, 45230, 16268])\n",
    "class_weights = 1. / train_counts\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for event_types, time_diffs, item_ids, labels in loader:\n",
    "        event_types = event_types.to(device)\n",
    "        time_diffs = time_diffs.to(device)\n",
    "        item_ids = item_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(event_types, time_diffs, item_ids)\n",
    "        # loss = F.cross_entropy(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * event_types.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for event_types, time_diffs, item_ids, labels in loader:\n",
    "            event_types = event_types.to(device)\n",
    "            time_diffs = time_diffs.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(event_types, time_diffs, item_ids)\n",
    "            # loss = F.cross_entropy(logits, labels)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item() * event_types.size(0)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    accuracy = correct / len(loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39e3e773-e42b-4416-8aa9-0907a019c19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=1.0608, Val Loss=1.0552, Val Acc=0.8491\n",
      "Epoch 2: Train Loss=1.0531, Val Loss=1.0697, Val Acc=0.7526\n",
      "Epoch 3: Train Loss=1.0499, Val Loss=1.0947, Val Acc=0.8412\n",
      "Epoch 4: Train Loss=1.0462, Val Loss=1.0735, Val Acc=0.8569\n",
      "Early stopping triggered after 4 epochs.\n",
      "Test Loss=1.0558, Test Acc=0.8598\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # Check if validation loss improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "# Load best model weights before testing\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Test evaluation\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8de91f-0e56-4b22-a3dc-0314581d48d2",
   "metadata": {},
   "source": [
    "# test the model with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71a1428e-a50f-4c44-8da0-e1f8cc25426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_event(model, user_events_seq, item2id, seq_len=50):\n",
    "    \"\"\"\n",
    "    Predict next event for a given user's event sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: trained BehaviorRNN model\n",
    "        user_events_seq: list of tuples (timestamp, event_type_str, item_id)\n",
    "            e.g. [(timestamp1, \"view\", item1), (timestamp2, \"addtocart\", item2), ...]\n",
    "        item2id: dict mapping item IDs to integer indices (same as training)\n",
    "        seq_len: length of sequence to feed model\n",
    "\n",
    "    Returns:\n",
    "        predicted_event: string label of predicted next event (\"view\", \"addtocart\", \"transaction\")\n",
    "        predicted_probs: softmax probabilities as numpy array\n",
    "    \"\"\"\n",
    "    # Map event string to int\n",
    "    event2id = {\"view\": 0, \"addtocart\": 1, \"transaction\": 2}\n",
    "    \n",
    "    # Convert raw events to training format: (timestamp, event_id, item_id)\n",
    "    processed_seq = [(ts, event2id.get(ev, 0), item) for ts, ev, item in user_events_seq]\n",
    "\n",
    "    # Use only last seq_len events, pad if needed\n",
    "    seq = processed_seq[-seq_len:]\n",
    "    pad_len = seq_len - len(seq)\n",
    "    if pad_len > 0:\n",
    "        seq = [(0, 0, 0)] * pad_len + seq\n",
    "    \n",
    "    timestamps = [e[0] for e in seq]\n",
    "    event_types = [e[1] for e in seq]\n",
    "    item_ids = [e[2] for e in seq]\n",
    "    time_diffs = [0] + [timestamps[j] - timestamps[j-1] for j in range(1, len(timestamps))]\n",
    "    \n",
    "    # Map item_ids using item2id vocab, default to 0 if unseen\n",
    "    item_ids_mapped = [item2id.get(iid, 0) for iid in item_ids]\n",
    "\n",
    "    # Create tensors\n",
    "    event_types_t = torch.tensor([event_types], dtype=torch.long).to(device)  # batch_size=1\n",
    "    time_diffs_t = torch.tensor([time_diffs], dtype=torch.float32).to(device)\n",
    "    item_ids_t = torch.tensor([item_ids_mapped], dtype=torch.long).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(event_types_t, time_diffs_t, item_ids_t)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        pred_id = np.argmax(probs)\n",
    "\n",
    "    id2event = {v: k for k, v in event2id.items()}\n",
    "    predicted_event = id2event[pred_id]\n",
    "\n",
    "    return predicted_event, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e39ebf08-7060-4c16-886c-f1593332df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next event: view\n",
      "Probabilities: [0.44539365 0.3790203  0.17558601]\n"
     ]
    }
   ],
   "source": [
    "# Example new user event sequence\n",
    "new_user_seq = [\n",
    "    (600000, \"view\", 601),               # Initial product view\n",
    "    (600030, \"view\", 602),               # Related product\n",
    "    (600060, \"view\", 601),               # Back to original product\n",
    "    (600090, \"addtocart\", 601),          # Adds it to cart\n",
    "    (600120, \"view\", 603),               # Still exploring alternatives\n",
    "    (600150, \"view\", 601), \n",
    "    (600180, \"addtocart\", 603),  \n",
    "]\n",
    "\n",
    "pred_event, pred_probs = predict_next_event(model, new_user_seq, item2id)\n",
    "print(\"Predicted next event:\", pred_event)\n",
    "print(\"Probabilities:\", pred_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef344474-3c29-416d-8d14-4776a1874106",
   "metadata": {},
   "source": [
    "# save the model locally and transferred to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d56b2260-5822-4325-967e-faf96c235c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"behavior_rnn_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37734d5d-7f85-4126-b51f-ae87560b1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model with the following code\n",
    "# model = torch.load(\"behavior_rnn_full.pth\")\n",
    "# model.to(device)\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f5292912-7f7b-4f21-8038-0d1f484c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !NOTE eval mode\n",
    "model.eval()\n",
    "\n",
    "# Create dummy inputs with the correct shape (batch_size=1, seq_len=50)\n",
    "dummy_event_types = torch.zeros((1, 50), dtype=torch.long).to(device)\n",
    "dummy_time_diffs = torch.zeros((1, 50), dtype=torch.float32).to(device)\n",
    "dummy_item_ids = torch.zeros((1, 50), dtype=torch.long).to(device)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_event_types, dummy_time_diffs, dummy_item_ids),\n",
    "    \"behavior_rnn.onnx\",\n",
    "    input_names=[\"event_types\", \"time_diffs\", \"item_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"event_types\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"time_diffs\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"item_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        \"logits\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=14,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "720b3b1e-0b33-4afd-99fe-5c4b63fdfd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " - event_types, shape: [0, 0]\n",
      " - time_diffs, shape: [0, 0]\n",
      " - item_ids, shape: [0, 0]\n",
      "Outputs:\n",
      " - logits, shape: [0, 3]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"behavior_rnn.onnx\")\n",
    "print(\"Inputs:\")\n",
    "for i in model.graph.input:\n",
    "    print(f\" - {i.name}, shape: {[dim.dim_value for dim in i.type.tensor_type.shape.dim]}\")\n",
    "\n",
    "print(\"Outputs:\")\n",
    "for o in model.graph.output:\n",
    "    print(f\" - {o.name}, shape: {[dim.dim_value for dim in o.type.tensor_type.shape.dim]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30672ca4-c02e-4b74-981f-895813e972d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %event_types[INT64, batch_sizexseq_len]\n",
      "  %time_diffs[FLOAT, batch_sizexseq_len]\n",
      "  %item_ids[INT64, batch_sizexseq_len]\n",
      ") initializers (\n",
      "  %event_embedding.weight[FLOAT, 3x32]\n",
      "  %item_embedding.weight[FLOAT, 150789x32]\n",
      "  %time_embedding.bias[FLOAT, 32]\n",
      "  %fc.weight[FLOAT, 3x64]\n",
      "  %fc.bias[FLOAT, 3]\n",
      "  %onnx::MatMul_103[FLOAT, 1x32]\n",
      "  %onnx::GRU_123[FLOAT, 1x192x96]\n",
      "  %onnx::GRU_124[FLOAT, 1x192x64]\n",
      "  %onnx::GRU_125[FLOAT, 1x384]\n",
      ") {\n",
      "  %/event_embedding/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/event_embedding/Gather_output_0 = Gather(%event_embedding.weight, %event_types)\n",
      "  %/item_embedding/Gather_output_0 = Gather(%item_embedding.weight, %item_ids)\n",
      "  %/Constant_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Unsqueeze_output_0 = Unsqueeze(%time_diffs, %/Constant_output_0)\n",
      "  %/time_embedding/MatMul_output_0 = MatMul(%/Unsqueeze_output_0, %onnx::MatMul_103)\n",
      "  %/time_embedding/Add_output_0 = Add(%time_embedding.bias, %/time_embedding/MatMul_output_0)\n",
      "  %/Concat_output_0 = Concat[axis = -1](%/event_embedding/Gather_output_0, %/time_embedding/Add_output_0, %/item_embedding/Gather_output_0)\n",
      "  %/rnn/Shape_output_0 = Shape(%/Concat_output_0)\n",
      "  %/rnn/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/rnn/Gather_output_0 = Gather[axis = 0](%/rnn/Shape_output_0, %/rnn/Constant_output_0)\n",
      "  %/rnn/Constant_1_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_29 = Constant[value = <Tensor>]()\n",
      "  %/rnn/Unsqueeze_output_0 = Unsqueeze(%/rnn/Gather_output_0, %onnx::Unsqueeze_29)\n",
      "  %/rnn/Constant_2_output_0 = Constant[value = <Tensor>]()\n",
      "  %/rnn/Concat_output_0 = Concat[axis = 0](%/rnn/Constant_1_output_0, %/rnn/Unsqueeze_output_0, %/rnn/Constant_2_output_0)\n",
      "  %/rnn/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%/rnn/Concat_output_0)\n",
      "  %/rnn/Transpose_output_0 = Transpose[perm = [1, 0, 2]](%/Concat_output_0)\n",
      "  %/rnn/GRU_output_0, %/rnn/GRU_output_1 = GRU[hidden_size = 64, linear_before_reset = 1](%/rnn/Transpose_output_0, %onnx::GRU_123, %onnx::GRU_124, %onnx::GRU_125, %, %/rnn/ConstantOfShape_output_0)\n",
      "  %/rnn/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/rnn/Squeeze_output_0 = Squeeze(%/rnn/GRU_output_0, %/rnn/Constant_3_output_0)\n",
      "  %/rnn/Transpose_1_output_0 = Transpose[perm = [1, 0, 2]](%/rnn/Squeeze_output_0)\n",
      "  %/Gather_output_0 = Gather[axis = 1](%/rnn/Transpose_1_output_0, %/event_embedding/Constant_output_0)\n",
      "  %logits = Gemm[alpha = 1, beta = 1, transB = 1](%/Gather_output_0, %fc.weight, %fc.bias)\n",
      "  return %logits\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"behavior_rnn.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71a50c-2228-41a8-a007-ae8a80bf02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "# Simple model that adds two numbers\n",
    "class AddModel(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.sum(dim=1, keepdim=True)  # Sum across columns\n",
    "\n",
    "# Instantiate model\n",
    "model = AddModel()\n",
    "model.eval()\n",
    "\n",
    "# Example input: batch of size 1 with 2 features\n",
    "dummy_input = torch.tensor([[3.0, 5.0]])\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    \"add_model.onnx\", \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"sum\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"sum\": {0: \"batch_size\"}},\n",
    "    opset_version=11\n",
    ")\n",
    "\n",
    "print(\"âœ… Model exported as 'add_model.onnx'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
